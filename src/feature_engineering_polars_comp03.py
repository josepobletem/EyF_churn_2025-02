# -*- coding: utf-8 -*-
"""
feature_engineering_polars
==========================

Genera variables derivadas (feature engineering) usando Polars:

- Sumas horizontales (saldos, consumos, inversiones, etc.).
- Lags y deltas (orden 1 y 2) para TODAS las variables num√©ricas,
  excluyendo las columnas clave (id, periodo, target).

Flujo:
1. Carga el dataset procesado con target (paths.processed_dataset en config.yaml).
2. Aplica feature engineering en Polars.
3. Guarda el dataset final en paths.feature_dataset (CSV o Parquet).

Uso:
    python -m src.feature_engineering_polars
"""

import os
import logging
from typing import List

import polars as pl
import yaml
from pydantic import BaseModel, Field, ValidationError
import gcsfs  # <--- para leer/escribir gs://

# -------------------------------------------------
# Logging
# -------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


# -------------------------------------------------
# Helpers para paths
# -------------------------------------------------

def is_gcs_path(path: str) -> bool:
    """Detecta si el path es de GCS (gs://...)."""
    return path.startswith("gs://")


# -------------------------------------------------
# Pydantic config models (compatibles con tu script actual)
# -------------------------------------------------

class PathsConfig(BaseModel):
    """
    PathsConfig
    -----------
    Rutas relevantes para el pipeline de datos.
    """
    raw_dataset: str
    processed_dataset: str
    feature_dataset: str = Field(
        ...,
        description="Ruta donde se guarda el dataset final con features para modelar."
    )


class ColumnsConfig(BaseModel):
    """
    ColumnsConfig
    -------------
    Columnas claves del dataset de clientes.
    """
    id_column: str
    period_column: str
    target_column: str = Field(
        "clase_ternaria",
        description="Nombre de la columna objetivo (ej. 'clase_ternaria')."
    )


class FeaturesConfig(BaseModel):
    """
    FeaturesConfig
    --------------
    Se deja por compatibilidad, aunque no sea estrictamente necesario
    en esta versi√≥n Polars.
    """
    base_table_name: str = Field(
        "base_clientes",
        description="Nombre l√≥gico del dataset base (solo informativo aqu√≠)."
    )
    steps: List[str] = Field(
        default_factory=list,
        description="No se usan en la versi√≥n Polars, pero se aceptan del YAML."
    )


class FullConfig(BaseModel):
    """
    FullConfig
    ----------
    Esquema completo esperado en config/config.yaml.
    """
    paths: PathsConfig
    columns: ColumnsConfig
    features: FeaturesConfig | None = None  # opcional para esta versi√≥n


def load_config(path: str = "config/config.yaml") -> FullConfig:
    """
    Cargar y validar la configuraci√≥n del proyecto usando Pydantic.
    """
    logger.info("Cargando configuraci√≥n de feature engineering (Polars) desde %s ...", path)

    if not os.path.exists(path):
        logger.error("No se encontr√≥ el archivo de configuraci√≥n: %s", path)
        raise FileNotFoundError(f"No encontr√© el archivo de configuraci√≥n {path}")

    with open(path, "r", encoding="utf-8") as f:
        raw_cfg = yaml.safe_load(f)

    try:
        cfg = FullConfig(**raw_cfg)
    except ValidationError as e:
        logger.error("Error validando config.yaml con Pydantic:\n%s", e)
        raise

    logger.debug("Config validada (feature engineering Polars): %s", cfg)
    return cfg


# -------------------------------------------------
# Definici√≥n de grupos de columnas (seg√∫n tu snippet)
# -------------------------------------------------

# Totales de saldos en pesos
SALDOS_PESOS = [
    "mcuenta_corriente",
    "mcaja_ahorro",
    "mcuenta_corriente_adicional",
    "mcaja_ahorro_adicional",
    "mcuentas_saldo",
]

# Totales de saldos en d√≥lares
SALDOS_DOLARES = [
    "mcaja_ahorro_dolares",
]

# Consumo tarjetas pesos
CONSUMO_TARJETAS_PESOS = [
    "mtarjeta_visa_consumo",
    "mtarjeta_master_consumo",
]

# Consumo tarjetas d√≥lares
CONSUMO_TARJETAS_DOLARES = [
    "Visa_mconsumosdolares",
    "Master_mconsumosdolares",
]

# Pagos recurrentes pesos
PAGOS_RECURRENTE_PESOS = [
    "mpagodeservicios",
    "mpagomiscuentas",
    "mcuenta_debitos_automaticos",
    "mttarjeta_visa_debitos_automaticos",
    "mttarjeta_master_debitos_automaticos",
]

# Sumas de productos de inversi√≥n y plazos fijos pesos
INVERSIONES_PESOS = [
    "mplazo_fijo_pesos",
    "minversion1_pesos",
    "minversion2",
]

# Sumas de productos de inversi√≥n y plazos fijos d√≥lares
INVERSIONES_DOLARES = [
    "mplazo_fijo_dolares",
    "minversion1_dolares",
]

# Endeudamiento total pesos
ENDEUDAMIENTO_PESOS = [
    "mprestamos_prendarios",
    "mprestamos_hipotecarios",
    "Master_msaldopesos",
    "Visa_msaldopesos",
    "Master_madelantopesos",
    "Visa_madelantopesos",
]

# Endeudamiento total d√≥lares
ENDEUDAMIENTO_DOLARES = [
    "Master_msaldodolares",
    "Visa_msaldodolares",
    "Master_madelantodolares",
    "Visa_madelantodolares",
]

# Ingresos payroll pesos
PAYROLL_PESOS = [
    "mpayroll",
    "mpayroll2",
]

# L√≠mites de compra
LIMITES_COMPRA = [
    "Master_mlimitecompra",
    "Visa_mlimitecompra",
]

# Suma de seguros contratados
SEGUROS_COUNT = [
    "cseguro_vida",
    "cseguro_auto",
    "cseguro_vivienda",
    "cseguro_accidentes_personales",
]

# Totales de transacciones (conteo)
TRANSACCIONES_COUNT = [
    "ctarjeta_debito_transacciones",
    "ctarjeta_visa_transacciones",
    "ctarjeta_master_transacciones",
    "chomebanking_transacciones",
    "cmobile_app_trx",
    "ccallcenter_transacciones",
    "catm_trx",
    "catm_trx_other",
    "ccajas_transacciones",
]

fe_columns = {
    "m_saldo_total_pesos": SALDOS_PESOS,
    "m_saldo_total_dolares": SALDOS_DOLARES,
    "m_consumo_tarjetas_pesos": CONSUMO_TARJETAS_PESOS,
    "m_consumo_tarjetas_dolares": CONSUMO_TARJETAS_DOLARES,
    "m_pagos_recurrentes_pesos": PAGOS_RECURRENTE_PESOS,
    "m_inversiones_pesos": INVERSIONES_PESOS,
    "m_inversiones_dolares": INVERSIONES_DOLARES,
    "m_endeudamiento_pesos": ENDEUDAMIENTO_PESOS,
    "m_endeudamiento_dolares": ENDEUDAMIENTO_DOLARES,
    "m_payroll_total": PAYROLL_PESOS,
    "m_limite_compra_total": LIMITES_COMPRA,
    "c_seguros_total": SEGUROS_COUNT,
    "c_transacciones_total": TRANSACCIONES_COUNT,
}


# -------------------------------------------------
# Funciones de feature engineering en Polars
# -------------------------------------------------

def sum_columns_safe(df: pl.DataFrame, cols: list[str]) -> pl.Expr:
    """
    Suma horizontal segura: ignora columnas que no existan en el DF.

    Si ninguna columna de `cols` existe, devuelve 0 literal.
    """
    existentes = [c for c in cols if c in df.columns]
    if not existentes:
        return pl.lit(0)
    return pl.sum_horizontal([pl.col(c) for c in existentes])


def add_lags_and_deltas(
    df: pl.DataFrame,
    id_col: str,
    period_col: str,
    cols: list[str],
) -> pl.DataFrame:
    """
    Agregar lags y deltas de orden 1 y 2 para todas las columnas num√©ricas indicadas.

    - lag1: x_{t-1}
    - lag2: x_{t-2}
    - d1: x_t - x_{t-1}
    - d2: x_t - x_{t-2}
    """
    logger.info(
        "Aplicando lags y deltas (lag1, lag2, d1, d2) a %d columnas num√©ricas...",
        len(cols),
    )

    out = df.sort([id_col, period_col]).lazy()

    # Lags 1 y 2
    out = out.with_columns([
        pl.col(c).shift(1).over(id_col).alias(f"{c}_lag1") for c in cols
    ] + [
        pl.col(c).shift(2).over(id_col).alias(f"{c}_lag2") for c in cols
    ])

    # Deltas
    out = out.with_columns([
        (pl.col(c) - pl.col(f"{c}_lag1")).alias(f"{c}_d1") for c in cols
    ] + [
        (pl.col(c) - pl.col(f"{c}_lag2")).alias(f"{c}_d2") for c in cols
    ])

    result = out.collect()
    logger.info("Lags y deltas agregados. Shape final: %s", (result.shape,))
    return result


def run_feature_engineering_polars() -> str:
    """
    Ejecuta la etapa de feature engineering usando Polars.

    Pasos:
    1. Carga el dataset procesado con target (`paths.processed_dataset`).
    2. Calcula features agregadas (sumas horizontales).
    3. Detecta todas las columnas num√©ricas (excluyendo id, periodo, target).
    4. Aplica lags y deltas (lag1, lag2, d1, d2) a todas esas columnas.
    5. Guarda el resultado en `paths.feature_dataset`.

    Returns
    -------
    str
        Ruta del dataset final de features.
    """
    cfg = load_config()
    processed_path = cfg.paths.processed_dataset
    feature_out_path = cfg.paths.feature_dataset

    id_col = cfg.columns.id_column
    period_col = cfg.columns.period_column
    target_col = cfg.columns.target_column

    logger.info("Dataset base procesado (con target): %s", processed_path)
    logger.info("Output final de features: %s", feature_out_path)
    logger.info("Columnas clave -> id: %s | periodo: %s | target: %s", id_col, period_col, target_col)

    # 1) Cargar dataset base en Polars (local o GCS)
    logger.info("Leyendo dataset base con Polars...")

    if is_gcs_path(processed_path):
        fs = gcsfs.GCSFileSystem()
        if processed_path.endswith(".parquet"):
            with fs.open(processed_path, "rb") as f:
                df = pl.read_parquet(f)
        else:
            with fs.open(processed_path, "rb") as f:
                df = pl.read_csv(f)
    else:
        if not os.path.exists(processed_path):
            logger.error("No se encontr√≥ el dataset procesado: %s", processed_path)
            raise FileNotFoundError(
                f"No encontr√© el archivo procesado (con target) {processed_path}"
            )

        if processed_path.endswith(".parquet"):
            df = pl.read_parquet(processed_path)
        else:
            df = pl.read_csv(processed_path)

    logger.info("Dataset base le√≠do. Shape: %s", (df.shape,))
    logger.debug("Columnas iniciales: %s", df.columns)

    # 2) Features agregadas (sumas horizontales)
    logger.info("Generando features agregadas (sumas horizontales)...")
    df = df.with_columns([
        sum_columns_safe(df, cols).alias(col_name)
        for col_name, cols in fe_columns.items()
    ])
    logger.info("Features agregadas creadas. Shape actual: %s", (df.shape,))

    # 3) Detectar columnas num√©ricas para lags/deltas (TODAS menos id, periodo, target)
    EXCLUDE = {id_col, period_col, target_col}
    num_cols = [
        c for c, dt in zip(df.columns, df.dtypes)
        if c not in EXCLUDE and dt.is_numeric()
    ]
    logger.info(
        "Columnas num√©ricas seleccionadas para lags/deltas: %d columnas.",
        len(num_cols),
    )
    logger.debug("Ejemplo de columnas num√©ricas: %s", num_cols[:20])

    # 4) Aplicar lags y deltas
    df = add_lags_and_deltas(df, id_col=id_col, period_col=period_col, cols=num_cols)

    # 5) Guardar resultado final (local o GCS)
    if is_gcs_path(feature_out_path):
        fs = gcsfs.GCSFileSystem()
        if feature_out_path.endswith(".parquet"):
            logger.info("Guardando features finales en Parquet (GCS): %s", feature_out_path)
            with fs.open(feature_out_path, "wb") as f:
                df.write_parquet(f)
        else:
            logger.info("Guardando features finales en CSV (GCS): %s", feature_out_path)
            with fs.open(feature_out_path, "wb") as f:
                df.write_csv(f)
    else:
        os.makedirs(os.path.dirname(feature_out_path), exist_ok=True)
        if feature_out_path.endswith(".parquet"):
            logger.info("Guardando features finales en Parquet: %s", feature_out_path)
            df.write_parquet(feature_out_path)
        else:
            logger.info("Guardando features finales en CSV: %s", feature_out_path)
            df.write_csv(feature_out_path)

    logger.info("Features finales guardadas en %s ‚úÖ", feature_out_path)
    return feature_out_path


if __name__ == "__main__":
    out_path = run_feature_engineering_polars()
    logger.info("üèÅ Feature engineering (Polars) completado. Output: %s", out_path)
